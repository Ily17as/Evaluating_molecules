{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nikita/edu/competitions/admet\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU, BatchNorm1d\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from lightning import pytorch as pl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "from chemprop import data, featurizers, models, nn\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train_admet.csv\", index_col=0)\n",
    "df_test = pd.read_csv(\"data/test_data.csv\", index_col=0)\n",
    "sample = pd.read_csv(\"data/sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7939/7939 [00:50<00:00, 158.02it/s]\n",
      "100%|██████████| 1221/1221 [00:08<00:00, 144.02it/s]\n"
     ]
    }
   ],
   "source": [
    "RDLogger.DisableLog(\"rdApp.*\")\n",
    "\n",
    "def get_decsriptors_df(smiles_list):\n",
    "    descriptors_list = []\n",
    "\n",
    "    for smiles in tqdm(smiles_list):\n",
    "        descriptors_list.append(\n",
    "            Descriptors.CalcMolDescriptors(Chem.MolFromSmiles(smiles), 0)\n",
    "        )\n",
    "    return pd.DataFrame(descriptors_list).fillna(0)\n",
    "\n",
    "train_descriptors = get_decsriptors_df(df_train[\"Drug\"])\n",
    "test_descriptors = get_decsriptors_df(df_test[\"Drug\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptors[\"Ipc\"] = np.log(train_descriptors[\"Ipc\"])\n",
    "test_descriptors[\"Ipc\"] = np.log(test_descriptors[\"Ipc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_descriptors)\n",
    "train_descriptors = pd.DataFrame(scaler.transform(train_descriptors), columns=train_descriptors.columns)\n",
    "test_descriptors = pd.DataFrame(scaler.transform(test_descriptors), columns=test_descriptors.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train, train_descriptors], axis=1)\n",
    "df_test = pd.concat([df_test, test_descriptors], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = train_descriptors.columns\n",
    "def get_descriptors_features(df):\n",
    "    return df[descriptors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trains = []\n",
    "df_vals = []\n",
    "df_tests = []\n",
    "properties = df_train.property.unique()\n",
    "\n",
    "for prop in properties:\n",
    "    subset_train = df_train[df_train.property == prop]\n",
    "    subset_train, subset_val = train_test_split(\n",
    "        subset_train, test_size=0.2, random_state=75, stratify=subset_train.Y\n",
    "    )\n",
    "    sampler = RandomOverSampler()\n",
    "    subset_train = sampler.fit_resample(subset_train, subset_train.Y)[0]\n",
    "    df_trains.append(subset_train)\n",
    "    df_vals.append(subset_val)\n",
    "    df_tests.append(df_test[df_test.property == prop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_total = []\n",
    "val_data_total = []\n",
    "test_data_total = []\n",
    "\n",
    "train_data_descriptors = []\n",
    "val_data_descriptors = []\n",
    "test_data_descriptors = []\n",
    "for i in range(len(df_trains)):\n",
    "    train_data_total.append(\n",
    "        [\n",
    "            data.MoleculeDatapoint.from_smi(smi, [y])\n",
    "            for smi, y in zip(df_trains[i][\"Drug\"], df_trains[i][\"Y\"])\n",
    "        ]\n",
    "    )\n",
    "    val_data_total.append(\n",
    "        [\n",
    "            data.MoleculeDatapoint.from_smi(smi, [y])\n",
    "            for smi, y in zip(df_vals[i][\"Drug\"], df_vals[i][\"Y\"])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_data_total.append(\n",
    "        [data.MoleculeDatapoint.from_smi(smi) for smi in df_tests[i][\"Drug\"]]\n",
    "    )\n",
    "\n",
    "    train_data_descriptors.append(get_descriptors_features(df_trains[i]).to_numpy().astype(np.float32))\n",
    "    val_data_descriptors.append(get_descriptors_features(df_vals[i]).to_numpy().astype(np.float32))\n",
    "    test_data_descriptors.append(get_descriptors_features(df_tests[i]).to_numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
    "train_datasets = [\n",
    "    data.MoleculeDataset(train_data, featurizer) for train_data in train_data_total\n",
    "]\n",
    "val_datasets = [\n",
    "    data.MoleculeDataset(val_data, featurizer) for val_data in val_data_total\n",
    "]\n",
    "test_datasets = [\n",
    "    data.MoleculeDataset(test_data, featurizer) for test_data in test_data_total\n",
    "]\n",
    "\n",
    "train_loaders = [\n",
    "    data.build_dataloader(train_dataset, shuffle=False, batch_size=32)\n",
    "    for train_dataset in train_datasets\n",
    "]\n",
    "val_loaders = [\n",
    "    data.build_dataloader(val_dataset, shuffle=False, batch_size=32)\n",
    "    for val_dataset in val_datasets\n",
    "]\n",
    "test_loaders = [\n",
    "    data.build_dataloader(test_dataset, shuffle=False, batch_size=32)\n",
    "    for test_dataset in test_datasets\n",
    "]\n",
    "\n",
    "train_feature_loaders = [\n",
    "    torch.utils.data.DataLoader(train_data_descriptors[i], batch_size=32, shuffle=False)\n",
    "    for i in range(len(train_datasets))\n",
    "]\n",
    "val_feature_loaders = [\n",
    "    torch.utils.data.DataLoader(val_data_descriptors[i], batch_size=32, shuffle=False)\n",
    "    for i in range(len(val_datasets))\n",
    "]\n",
    "test_feature_loaders = [\n",
    "    torch.utils.data.DataLoader(test_data_descriptors[i], batch_size=32, shuffle=False)\n",
    "    for i in range(len(test_datasets))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(self, *loaders):\n",
    "        self.loaders = loaders\n",
    "\n",
    "    def __iter__(self):\n",
    "        for item in zip(*self.loaders):\n",
    "            yield item\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(loader) for loader in self.loaders)\n",
    "\n",
    "\n",
    "train_combined_loaders = [\n",
    "    CombinedLoader(train_loaders[i], train_feature_loaders[i])\n",
    "    for i in range(len(train_loaders))\n",
    "]\n",
    "val_combined_loaders = [\n",
    "    CombinedLoader(val_loaders[i], val_feature_loaders[i])\n",
    "    for i in range(len(val_loaders))\n",
    "]\n",
    "test_combined_loaders = [\n",
    "    CombinedLoader(test_loaders[i], test_feature_loaders[i])\n",
    "    for i in range(len(test_loaders))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeCrusher(torch.nn.Module):\n",
    "    def __init__(self, mpnn, input_dim, embedding_dim=256):\n",
    "        super().__init__()\n",
    "        self.mpnn = mpnn\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedder = Sequential(\n",
    "            Linear(input_dim, embedding_dim),\n",
    "            ReLU(),\n",
    "            Linear(embedding_dim, embedding_dim),\n",
    "            ReLU(),\n",
    "            Linear(embedding_dim, embedding_dim),\n",
    "        )\n",
    "\n",
    "        prev_in_features = mpnn.predictor.ffn[0][0].in_features\n",
    "        prev_out_features = mpnn.predictor.ffn[0][0].out_features\n",
    "        self.mpnn.predictor.ffn[0][0] = Linear(\n",
    "            prev_in_features + embedding_dim, prev_out_features\n",
    "        )\n",
    "\n",
    "        self.bn = BatchNorm1d(embedding_dim + prev_in_features)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-4, weight_decay=2e-5)\n",
    "\n",
    "        # self.initialize_weights()\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def loss(self, pred, target):\n",
    "        return F.binary_cross_entropy(pred, target, reduction=\"mean\")\n",
    "\n",
    "    def metric(self, pred, target):\n",
    "        return roc_auc_score(target, pred)\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, bmg: data.collate.BatchMolGraph, features: torch.Tensor):\n",
    "        features_embedding = self.embedder(features)\n",
    "        mol_embedding = self.mpnn.agg(self.mpnn.message_passing(bmg), bmg.batch)\n",
    "\n",
    "        embedding = self.bn(torch.cat([mol_embedding, features_embedding], dim=1))\n",
    "\n",
    "        return self.mpnn.predictor(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnns = []\n",
    "for i in range(len(train_datasets)):\n",
    "    mp = nn.BondMessagePassing()\n",
    "    agg = nn.MeanAggregation()\n",
    "    ffn = nn.BinaryClassificationFFN()\n",
    "    batch_norm = True\n",
    "    metric_list = [\n",
    "        nn.metrics.BinaryAUROCMetric(),\n",
    "        nn.metrics.BinaryAccuracyMetric(),\n",
    "        nn.metrics.BCEMetric(),\n",
    "    ]\n",
    "\n",
    "    mpnn = models.MPNN(mp, agg, ffn, batch_norm, metric_list)\n",
    "    mpnns.append(mpnn)\n",
    "\n",
    "mc_models = [MoleculeCrusher(mpnns[i], 210, 256) for i in range(len(mpnns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs, model_name):\n",
    "    best_score = -float(\"inf\")\n",
    "    best_it = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loop = tqdm(train_loader, desc=f\"Train epoch {epoch}\")\n",
    "        losses = []\n",
    "        for batch in train_loop:\n",
    "            batch_mol, batch_feat = batch\n",
    "            bmg, V_d, X_d, target, weights, lt_mask, gt_mask = batch_mol\n",
    "\n",
    "            bmg.V = bmg.V.to(model.device)\n",
    "            bmg.E = bmg.E.to(model.device)\n",
    "            bmg.edge_index = bmg.edge_index.to(model.device)\n",
    "            bmg.rev_edge_index = bmg.rev_edge_index.to(model.device)\n",
    "            bmg.batch = bmg.batch.to(model.device)\n",
    "            target = target.to(model.device)\n",
    "            batch_feat = batch_feat.to(model.device)\n",
    "            pred = model.forward(bmg, batch_feat)\n",
    "\n",
    "            model.optimizer.zero_grad()\n",
    "            loss = model.loss(pred, target)\n",
    "            loss.backward()\n",
    "            model.optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            train_loop.set_postfix(loss=np.mean(losses))\n",
    "\n",
    "        model.eval()\n",
    "        val_loop = tqdm(val_loader, desc=f\"Val epoch {epoch}\")\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        for batch in val_loop:\n",
    "            batch_mol, batch_feat = batch\n",
    "            bmg, V_d, X_d, target, weights, lt_mask, gt_mask = batch_mol\n",
    "\n",
    "            bmg.V = bmg.V.to(model.device)\n",
    "            bmg.E = bmg.E.to(model.device)\n",
    "            bmg.edge_index = bmg.edge_index.to(model.device)\n",
    "            bmg.rev_edge_index = bmg.rev_edge_index.to(model.device)\n",
    "            bmg.batch = bmg.batch.to(model.device)\n",
    "            target = target.to(model.device)\n",
    "            batch_feat = batch_feat.to(model.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = model.forward(bmg, batch_feat)\n",
    "\n",
    "            all_preds.extend(pred.view(-1).tolist())\n",
    "            all_targets.extend(target.view(-1).tolist())\n",
    "\n",
    "        roc_auc = roc_auc_score(all_targets, all_preds)\n",
    "        if roc_auc > best_score:\n",
    "            best_score = roc_auc\n",
    "            best_it = epoch\n",
    "\n",
    "        print(f\"Validation ROC AUC: {roc_auc}\")\n",
    "        torch.save(model.state_dict(), f\"checkpoints/{model_name}_{epoch}.pt\")\n",
    "    print(f\"Best score: {best_score} at iteration {best_it}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 0: 100%|██████████| 138/138 [00:06<00:00, 22.97it/s, loss=0.572]\n",
      "Val epoch 0: 100%|██████████| 32/32 [00:00<00:00, 61.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8344893399124849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 1: 100%|██████████| 138/138 [00:04<00:00, 29.45it/s, loss=0.493]\n",
      "Val epoch 1: 100%|██████████| 32/32 [00:00<00:00, 64.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8631645098221767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 2: 100%|██████████| 138/138 [00:05<00:00, 27.54it/s, loss=0.44] \n",
      "Val epoch 2: 100%|██████████| 32/32 [00:00<00:00, 62.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8689600595847687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 3: 100%|██████████| 138/138 [00:05<00:00, 27.53it/s, loss=0.393]\n",
      "Val epoch 3: 100%|██████████| 32/32 [00:00<00:00, 54.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.873626757285169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 4: 100%|██████████| 138/138 [00:04<00:00, 28.58it/s, loss=0.347]\n",
      "Val epoch 4: 100%|██████████| 32/32 [00:00<00:00, 57.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8717841293486019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 5: 100%|██████████| 138/138 [00:04<00:00, 28.49it/s, loss=0.302]\n",
      "Val epoch 5: 100%|██████████| 32/32 [00:00<00:00, 46.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8694992707072589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 6: 100%|██████████| 138/138 [00:04<00:00, 27.70it/s, loss=0.258]\n",
      "Val epoch 6: 100%|██████████| 32/32 [00:00<00:00, 56.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8693634981224592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 7: 100%|██████████| 138/138 [00:04<00:00, 30.22it/s, loss=0.218]\n",
      "Val epoch 7: 100%|██████████| 32/32 [00:00<00:00, 48.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8607826707631197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 8: 100%|██████████| 138/138 [00:04<00:00, 29.43it/s, loss=0.182]\n",
      "Val epoch 8: 100%|██████████| 32/32 [00:00<00:00, 66.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.858881854575924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 9: 100%|██████████| 138/138 [00:04<00:00, 30.84it/s, loss=0.15] \n",
      "Val epoch 9: 100%|██████████| 32/32 [00:00<00:00, 52.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8512165223598052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 10: 100%|██████████| 138/138 [00:05<00:00, 26.59it/s, loss=0.128]\n",
      "Val epoch 10: 100%|██████████| 32/32 [00:00<00:00, 57.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8359576234366757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 11: 100%|██████████| 138/138 [00:05<00:00, 23.81it/s, loss=0.11] \n",
      "Val epoch 11: 100%|██████████| 32/32 [00:00<00:00, 47.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8240115755826584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 12: 100%|██████████| 138/138 [00:05<00:00, 25.81it/s, loss=0.0996]\n",
      "Val epoch 12: 100%|██████████| 32/32 [00:00<00:00, 48.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.839167675263011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 13: 100%|██████████| 138/138 [00:05<00:00, 26.06it/s, loss=0.0984]\n",
      "Val epoch 13: 100%|██████████| 32/32 [00:00<00:00, 59.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8575512832448873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 14: 100%|██████████| 138/138 [00:05<00:00, 27.04it/s, loss=0.0931]\n",
      "Val epoch 14: 100%|██████████| 32/32 [00:00<00:00, 51.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.850615243769978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 15: 100%|██████████| 138/138 [00:05<00:00, 26.04it/s, loss=0.0658]\n",
      "Val epoch 15: 100%|██████████| 32/32 [00:00<00:00, 53.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8388321230177203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 16: 100%|██████████| 138/138 [00:05<00:00, 25.70it/s, loss=0.052] \n",
      "Val epoch 16: 100%|██████████| 32/32 [00:00<00:00, 50.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8284920708810476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 17: 100%|██████████| 138/138 [00:05<00:00, 26.99it/s, loss=0.0457]\n",
      "Val epoch 17: 100%|██████████| 32/32 [00:00<00:00, 62.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8302086242745865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 18: 100%|██████████| 138/138 [00:04<00:00, 28.15it/s, loss=0.0433]\n",
      "Val epoch 18: 100%|██████████| 32/32 [00:00<00:00, 48.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8209974242001056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 19: 100%|██████████| 138/138 [00:04<00:00, 28.04it/s, loss=0.0444]\n",
      "Val epoch 19: 100%|██████████| 32/32 [00:00<00:00, 56.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8303288799925518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 20: 100%|██████████| 138/138 [00:05<00:00, 27.20it/s, loss=0.0492]\n",
      "Val epoch 20: 100%|██████████| 32/32 [00:00<00:00, 57.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.834855925891444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 21: 100%|██████████| 138/138 [00:05<00:00, 25.53it/s, loss=0.0531]\n",
      "Val epoch 21: 100%|██████████| 32/32 [00:00<00:00, 53.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8292252428389661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 22: 100%|██████████| 138/138 [00:05<00:00, 25.40it/s, loss=0.0426]\n",
      "Val epoch 22: 100%|██████████| 32/32 [00:00<00:00, 62.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8367567420786395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 23: 100%|██████████| 138/138 [00:05<00:00, 26.39it/s, loss=0.0405]\n",
      "Val epoch 23: 100%|██████████| 32/32 [00:00<00:00, 60.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8467321478447073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 24: 100%|██████████| 138/138 [00:05<00:00, 25.05it/s, loss=0.0516]\n",
      "Val epoch 24: 100%|██████████| 32/32 [00:00<00:00, 54.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.8711246625081464\n",
      "Best score: 0.873626757285169 at iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 0: 100%|██████████| 62/62 [00:03<00:00, 16.20it/s, loss=0.691]\n",
      "Val epoch 0: 100%|██████████| 9/9 [00:00<00:00, 30.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.5652040816326531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 1: 100%|██████████| 62/62 [00:03<00:00, 16.52it/s, loss=0.659]\n",
      "Val epoch 1: 100%|██████████| 9/9 [00:00<00:00, 36.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.6100000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 2:  60%|█████▉    | 37/62 [00:02<00:01, 18.99it/s, loss=0.58] "
     ]
    }
   ],
   "source": [
    "for i in range(len(mc_models)):\n",
    "    train(\n",
    "        mc_models[i], train_combined_loaders[i], val_combined_loaders[i], 25, f\"mpnn{i}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133269/1140434547.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mc_models[0].load_state_dict(torch.load(\"checkpoints/mpnn0_18.pt\"))\n",
      "/tmp/ipykernel_133269/1140434547.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mc_models[1].load_state_dict(torch.load(\"checkpoints/mpnn1_18.pt\"))\n",
      "/tmp/ipykernel_133269/1140434547.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mc_models[2].load_state_dict(torch.load(\"checkpoints/mpnn2_10.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_models[0].load_state_dict(torch.load(\"checkpoints/mpnn0_18.pt\"))\n",
    "mc_models[1].load_state_dict(torch.load(\"checkpoints/mpnn1_18.pt\"))\n",
    "mc_models[2].load_state_dict(torch.load(\"checkpoints/mpnn2_10.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in mc_models:\n",
    "    model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "for i in range(len(test_datasets)):\n",
    "    property_preds = []\n",
    "    for batch in test_combined_loaders[i]:\n",
    "        batch_mol, batch_feat = batch\n",
    "        bmg, V_d, X_d, target, weights, lt_mask, gt_mask = batch_mol\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = mc_models[i].forward(bmg, batch_feat)\n",
    "        property_preds.extend(pred.view(-1).tolist())\n",
    "    test_preds.append(property_preds)\n",
    "\n",
    "test_preds = sum(test_preds, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"Y\"] = test_preds\n",
    "sample.to_csv(\"submissions/chemprop_multi.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
